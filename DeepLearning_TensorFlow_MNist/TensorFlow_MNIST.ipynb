{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSyt19GZ5zO2"
      },
      "source": [
        "# **Deep Neural Network for MNIST**\n",
        "\n",
        "Classification\n",
        "\n",
        "We'll apply all the knowledge from the lectures in this section to write a deep neural network. The problem we've chosen is referred to as the \"Hello World\" of deep learning because for most students it is the first deep learning algorithm they see.\n",
        "\n",
        "The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional neural networks (CNNs).\n",
        "\n",
        "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image).\n",
        "\n",
        "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes.\n",
        "\n",
        "Our goal would be to build a neural network with 2 hidden layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-6-Nein5zO3"
      },
      "source": [
        "## **Import the relevant packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-yAfSkix5zO3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mDSzLwl5zO4"
      },
      "source": [
        "## **Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-w3Yo9kb5zO4"
      },
      "outputs": [],
      "source": [
        "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
        "\n",
        "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
        "\n",
        "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
        "\n",
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32) # 转换为 float32，float是带小数点，精度大约是小数点后7位有效数字。\n",
        "  image /= 255.                      # 像素值缩放到 [0,1]\n",
        "  return image, label\n",
        "\n",
        "scaled_train_and_validation_data = mnist_train.map(scale)\n",
        "\n",
        "test_data = mnist_test.map(scale)\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
        "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "train_data = train_data.batch(BATCH_SIZE)\n",
        "validation_data = validation_data.batch(num_validation_samples)\n",
        "\n",
        "test_data = test_data.batch(BATCH_SIZE)\n",
        "\n",
        "validation_inputs, validation_targets = next(iter(validation_data))"
      ],
      "metadata": {
        "id": "gbq0PQOYBU17"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**tfds.load()** tfds = TensorFlow Datasets 库，里面收录了很多常见的数据集（MNIST, CIFAR-10, IMDB, Fashion-MNIST 等）。\n",
        "\n",
        "tfds.load() 会自动下载并准备数据集。\n",
        "\n",
        "**name='mnist'**\n",
        "指定要下载的数据集是 MNIST。\n",
        "\n",
        "**with_info=True**\n",
        "返回一个 tfds.core.DatasetInfo 对象（这里是 mnist_info），里面包含：\n",
        "\n",
        "数据集的元信息（训练集大小、测试集大小、类别标签数、图像大小等）。\n",
        "\n",
        "例如 mnist_info.splits['train'].num_examples 可以得到训练样本数 = 60000。\n",
        "\n",
        "**as_supervised=True**\n",
        "数据集会以 (input, label) 形式返回。\n",
        "\n",
        "input：28×28 灰度图（Tensor）。\n",
        "\n",
        "label：对应的数字（0–9 的整数）。\n",
        "如果不用 as_supervised，返回的是字典格式（更冗余）。\n",
        "\n",
        "**mnist_train** → 训练集 (60,000 张)。\n",
        "\n",
        "**mnist_test** → 测试集 (10,000 张)。"
      ],
      "metadata": {
        "id": "Pr1K8lmr8GTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST 训练集共有 60,000 张。\n",
        "\n",
        "**num_validation_samples** 取 10% (6,000 张) 作为验证集。\n",
        "\n",
        "**tf.cast(..., tf.int64):** 确保数据类型是 TensorFlow 的 int64（方便后面 take() 和 skip() 用）。\n",
        "\n",
        "**int64:** 64 位整数。把 6000.0 转换为 TensorFlow 的 64 位整数，变成 6000。\n",
        "\n",
        "**mnist_train.map(scale), mnist_test.map** 用map()将mnist_train里面的数据通过scale function转换成float32，缩放到[0,1]"
      ],
      "metadata": {
        "id": "0rk98oKQDQ3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**shuffle(BUFFER_SIZE)：** 把数据打乱，避免模型训练时顺序带来偏差。\n",
        "\n",
        "**BUFFER_SIZE=10000：** 并不是把所有数据一次性打乱，而是维持一个 缓冲区，里面随机采样数据。一般 BUFFER_SIZE 越大，打乱效果越好（但更耗内存）。"
      ],
      "metadata": {
        "id": "BEfMY8dPgXi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**take(num_validation_samples)：** 取前 num_validation_samples（6000） 个样本作为验证集。\n",
        "\n",
        "**skip(num_validation_samples)：** 跳过这部分，剩下的作为训练集（54000）。\n",
        "\n",
        "因为之前做过 shuffle，所以 取前6000并不会有顺序偏差，相当于随机划分。"
      ],
      "metadata": {
        "id": "RX6bXQ0swJTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**train_data.batch(100)：训练集**会被分成 每批100个样本，比如 (100, 28, 28, 1) → (100 个图片, 每个28×28, 单通道)。\n",
        "\n",
        "**validation_data.batch(num_validation_samples)：验证集**直接设为 一个大批次（6000个），这样在验证时就能一次性取出所有验证数据。\n",
        "\n",
        "**test_data.batch(BATCH_SIZE)测试集**也是分成每批100个样本"
      ],
      "metadata": {
        "id": "G562ALat9sef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**validation_inputs, validation_targets = next(iter(validation_data))**\n",
        "\n",
        "validation_data 是一个 dataset，每次迭代返回 (images, labels)。因为前面加载 MNIST 时用了tfds.load。参数 as_supervised=True 表示数据会以 (input, label)的形式返回：\n",
        "\n",
        "input → 一张图像 (28x28x1 的 tensor)\n",
        "\n",
        "label → 这张图像对应的数字 (0~9 的整数)\n",
        "\n",
        "再加上后面做了 .batch()，所以 每次迭代的时候不是返回一张图，而是返回一批数据：\n",
        "\n",
        "images → 一批图片，形状大概是 (batch_size, 28, 28, 1)\n",
        "\n",
        "labels → 一批对应的标签，形状 (batch_size,)\n",
        "\n",
        "**next(iter())**\n",
        "\n",
        "iter是生成一个迭代器，然后让next从迭代器里取出一批一批地按顺序读取数据\n",
        "\n",
        "next() 取出一个批次 (images, labels)。\n",
        "\n",
        "iter(validation_data) → 生成一个迭代器，表示“按批次读取验证集”。"
      ],
      "metadata": {
        "id": "gGaiK794-LNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Outline the model**"
      ],
      "metadata": {
        "id": "0IAYYHK3m0Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 784\n",
        "output_size = 10\n",
        "hidden_layer_size = 50\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
        "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSfI6d7Hmz2w",
        "outputId": "c63ef48e-6393-4de9-e3cc-8337286d1b18"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**input_size：** 每张图片 28x28 = 784 个像素。\n",
        "\n",
        "**output_size：** 模型输出 10 个节点，对应数字 0~9 的类别概率。\n",
        "\n",
        "**hidden_layer_size：** 隐藏层有 50 个神经元，可以调节网络容量。"
      ],
      "metadata": {
        "id": "mLyXqHkTO_C-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**tf.keras.layers.Flatten(input_shape=(28,28,1))**\n",
        "\n",
        "将 28x28x1 的图像拉成 784 维向量。\n",
        "\n",
        "MLP 的输入必须是一维向量，所以需要 Flatten。每一层的神经元是全连接的，也就是说上一层的每个输出都要连接到下一层的每个神经元。\n",
        "\n",
        "如果输入是二维（28x28 图片），网络不知道怎么直接全连接，所以必须把它“拉平”成 一维向量（784 维）。\n",
        "\n",
        "Flatten 层就是把 28x28x1 → 784，就可以送进全连接层。"
      ],
      "metadata": {
        "id": "t8jWJNFnRrER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**activation='relu'**\n",
        "\n",
        "ReLU（Rectified Linear Unit）非线性激活函数\n",
        "\n",
        "特点：\n",
        "\n",
        "正数保持不变，负数变 0\n",
        "\n",
        "训练快，解决梯度消失问题\n",
        "\n",
        "适用场景：\n",
        "\n",
        "隐藏层神经元，大多数深度神经网络默认用 ReLU\n",
        "\n",
        "CNN、MLP 等隐藏层"
      ],
      "metadata": {
        "id": "u1U5PvtqX5BD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**activation='softmax'**\n",
        "\n",
        "**概率分布**\n",
        "\n",
        "MNIST 是多分类问题（数字 0–9 共 10 类）\n",
        "\n",
        "Softmax 会把每个神经元的输出转成 0~1 的概率，并且 所有输出加起来等于 1\n",
        "\n",
        "这样可以直接用交叉熵损失函数（sparse_categorical_crossentropy）训练\n",
        "\n",
        "**方便判断类别**\n",
        "\n",
        "输出概率最大的那个神经元就是预测的类别。\n",
        "\n",
        "例如 [0.01, 0.02, 0.90, ...]（概率 0.9 最大）。"
      ],
      "metadata": {
        "id": "dLBGCCPJaW7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Choose the optimizer and the loss function**"
      ],
      "metadata": {
        "id": "HemfSLW8n798"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "vrC-oW1vn7SQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**optimizer='adam'** 自适应梯度下\n",
        "\n",
        "Adam自适应学习率：每个参数都有自己的学习率，自动调整。动量机制：结合过去梯度的信息，加速收敛。\n",
        "\n",
        "**loss='sparse_categorical_crossentropy'**  计算衡量误差\n",
        "\n",
        "通过One-hot预测出概率后，用Softmax 会把每个神经元的输出转成 0~1 的概率，并且 所有输出加起来等于 1。最后用sparse_categorical_crossentropy计算衡量 Softmax 输出概率和真实类别之间差距，用于训练模型\n",
        "\n",
        "直观理解\n",
        "\n",
        "交叉熵 = 衡量“预测概率分布和真实标签分布的差距”\n",
        "\n",
        "预测越接近真实类别 → 损失越小\n",
        "\n",
        "预测离真实类别越远 → 损失越大"
      ],
      "metadata": {
        "id": "7kQbu2YpbZeH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "V9WKiBbdpwqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 5\n",
        "\n",
        "model.fit(train_data, epochs = NUM_EPOCHS, validation_data=(validation_inputs, validation_targets), verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZDlnKHfpw8S",
        "outputId": "ac62a575-61cc-4c67-e374-0e605e487968"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "540/540 - 10s - 19ms/step - accuracy: 0.8836 - loss: 0.4082 - val_accuracy: 0.9385 - val_loss: 0.2130\n",
            "Epoch 2/5\n",
            "540/540 - 10s - 18ms/step - accuracy: 0.9471 - loss: 0.1819 - val_accuracy: 0.9520 - val_loss: 0.1607\n",
            "Epoch 3/5\n",
            "540/540 - 6s - 11ms/step - accuracy: 0.9592 - loss: 0.1378 - val_accuracy: 0.9600 - val_loss: 0.1347\n",
            "Epoch 4/5\n",
            "540/540 - 6s - 10ms/step - accuracy: 0.9666 - loss: 0.1132 - val_accuracy: 0.9690 - val_loss: 0.1069\n",
            "Epoch 5/5\n",
            "540/540 - 4s - 8ms/step - accuracy: 0.9712 - loss: 0.0948 - val_accuracy: 0.9743 - val_loss: 0.0946\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7bdc68378f50>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Epoch** = 训练整个数据集一次\n",
        "\n",
        "**NUM_EPOCHS = 5** → 整个训练集会被完整训练 5 次\n",
        "\n",
        "**train_data** → 训练集（已经 batch 好，每批 100 张图像）\n",
        "\n",
        "**epochs=NUM_EPOCHS** → 训练轮数(5)\n",
        "\n",
        "**validation_data=(validation_inputs, validation_targets)** → 验证集，用于每轮训练后评估模型效果，不参与训练\n",
        "\n",
        "**verbose=2** → 输出训练进度：\n",
        "\n",
        "每个 epoch 输出训练和验证的 loss/accuracy"
      ],
      "metadata": {
        "id": "HoJqRMLoi2j9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test the model**"
      ],
      "metadata": {
        "id": "EBa2mV4KqmFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiWq40UjqmVn",
        "outputId": "36b7055b-8352-49d0-c3a2-aeb40db87f1a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9667 - loss: 0.1056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdrzhN77smD2",
        "outputId": "20418faa-c949-469b-8952-1165b0aa7272"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.11. Test accuracy: 96.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**test_data** → 测试集（未参与训练和验证）\n",
        "\n",
        "**evaluate()** 会：\n",
        "\n",
        "1. 前向传播整个测试集\n",
        "\n",
        "2. 计算损失（test_loss）\n",
        "\n",
        "3. 计算准确率（test_accuracy）\n",
        "\n",
        "测试集指标能反映模型的 泛化能力（在新数据上的表现）"
      ],
      "metadata": {
        "id": "A58QYffjjI52"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:py3-TF2.0]",
      "language": "python",
      "name": "conda-env-py3-TF2.0-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}